{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCNm0LALcOYp8+d+dLqJhu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadAbdullah80/Velocity-Solutions/blob/main/Week_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üß© Step 1 ‚Äì Loading the MNIST Dataset\n"
      ],
      "metadata": {
        "id": "jqMtXNhXRHBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We used the built-in `tensorflow.keras.datasets` module to load the MNIST dataset. It contains **70,000 grayscale images** of handwritten digits (0‚Äì9), each of size 28√ó28 pixels.\n",
        "\n",
        "\n",
        "#### üìå What I Learned:\n",
        "- How to load a popular dataset using TensorFlow.\n",
        "- `x_train` and `x_test` are arrays of images.\n",
        "- `y_train` and `y_test` are arrays of corresponding digit labels (0‚Äì9).\n",
        "\n"
      ],
      "metadata": {
        "id": "BdF1Vo3IQNf-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "peQASvoZmE75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d58aec-a817-4402-ec87-b649e121db63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training data: (60000, 28, 28)\n",
            "Training labels: (60000,)\n",
            "Test data: (10000, 28, 28)\n",
            "Test labels: (10000,)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Check shapes\n",
        "print(\"Training data:\", x_train.shape)\n",
        "print(\"Training labels:\", y_train.shape)\n",
        "print(\"Test data:\", x_test.shape)\n",
        "print(\"Test labels:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß© Step 2 ‚Äì Data Visualization and Preprocessing"
      ],
      "metadata": {
        "id": "JKVk57h-TjK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In this step, I visualized the MNIST images and prepared the data for use with machine learning models.\n",
        "\n",
        "---\n",
        "\n",
        "#### üîç Sample Visualization\n",
        "\n",
        "To get familiar with the dataset, I displayed a few sample images from the training data. This helped me understand how the handwritten digits look and verify that the data was loaded correctly.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚öôÔ∏è Data Preprocessing\n",
        "\n",
        "1. **Normalization**  \n",
        "   Pixel values originally range from 0 to 255. I scaled them down to a 0‚Äì1 range to ensure more stable and faster model training. This step also avoids issues where certain pixel values dominate due to their larger scale.\n",
        "\n",
        "2. **Flattening**  \n",
        "   Each image in the dataset is 28√ó28 pixels. I converted these into 1D vectors of length 784 so that models like k-Nearest Neighbors and Support Vector Machines can process them.\n",
        "\n",
        "---\n",
        "\n",
        "#### üßæ Output:\n",
        "\n",
        "```\n",
        "Flattened training data: (60000, 784)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### üìå What I Learned:\n",
        "\n",
        "- Normalizing pixel values improves model performance and speeds up training.\n",
        "- Flattening 2D images into 1D vectors is necessary for non-CNN models.\n",
        "- Visualization helps build intuition about the dataset before model development.\n",
        "\n"
      ],
      "metadata": {
        "id": "t52nOJCNRc-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Show first 9 images\n",
        "plt.figure(figsize=(6, 6))\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.imshow(x_train[i], cmap='gray')\n",
        "    plt.title(f\"Label: {y_train[i]}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "VAFQElGSNz39",
        "outputId": "64332e35-fe1c-4b06-d13e-5c4276a34794"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Normalize pixel values to 0‚Äì1\n",
        "x_train_norm = x_train / 255.0\n",
        "x_test_norm = x_test / 255.0\n",
        "\n",
        "# 2. Flatten 28x28 images to 1D (for non-CNN models like kNN or SVM)\n",
        "x_train_flat = x_train_norm.reshape(-1, 28*28)\n",
        "x_test_flat = x_test_norm.reshape(-1, 28*28)\n",
        "\n",
        "print(\"Flattened training data:\", x_train_flat.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aejw7AJ5Relp",
        "outputId": "f4d51c59-58b4-4388-8cbd-c7f874f174ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened training data: (60000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öôÔ∏è STEP 3 - Training a k-Nearest Neighbors (kNN) or Support Vector Machine (SVM) model"
      ],
      "metadata": {
        "id": "do8Dvn0D7FP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification with k-Nearest Neighbors (kNN)\n",
        "\n",
        "In this step, I implemented and trained a **k-Nearest Neighbors (kNN)** model to classify handwritten digits from the MNIST dataset.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚öôÔ∏è Model Used:  \n",
        "**k-Nearest Neighbors (kNN)** ‚Äî a simple, instance-based learning algorithm that predicts the class of a data point based on the majority class of its k nearest neighbors.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß™ Setup:\n",
        "- Used only the first **10,000 training images** (instead of all 60,000) for faster runtime.\n",
        "- Flattened each 28√ó28 image into a 784-dimensional vector.\n",
        "- Set `k = 3` (used 3 nearest neighbors to make predictions).\n",
        "\n",
        "---\n",
        "\n",
        "#### üìä Results:\n",
        "- Achieved an accuracy of around **94.6%** on the test set.\n",
        "- Classification report showed strong performance across all digits.\n",
        "- Observed that smaller `k` values can be more sensitive to noise, while larger values can smooth out results but may miss fine details.\n",
        "\n",
        "---\n",
        "\n",
        "#### üìå What I Learned:\n",
        "- kNN is easy to implement but **computationally expensive at prediction time**.\n",
        "- Works well on small-to-medium datasets, especially when classes are well-separated.\n",
        "- Choosing the right value of **k** is crucial for balancing bias and variance.\n",
        "\n"
      ],
      "metadata": {
        "id": "XnZ8Jj9Vy1Bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# 1. Use a smaller training subset for speed\n",
        "x_train_sample = x_train_flat[:10000]\n",
        "y_train_sample = y_train[:10000]\n",
        "\n",
        "# 2. Initialize and train the model\n",
        "knn = KNeighborsClassifier(n_neighbors=3)  # You can change k here\n",
        "knn.fit(x_train_sample, y_train_sample)\n",
        "\n",
        "# 3. Predict on test set\n",
        "y_pred_knn = knn.predict(x_test_flat)\n",
        "\n",
        "# 4. Evaluate the model\n",
        "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "print(\"kNN Accuracy:\", accuracy_knn)\n",
        "\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred_knn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-7Ahc_xSRGW",
        "outputId": "c2f322a4-8852-4b68-d056-492c45651e26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kNN Accuracy: 0.9463\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.99      0.97       980\n",
            "           1       0.93      0.99      0.96      1135\n",
            "           2       0.97      0.93      0.95      1032\n",
            "           3       0.93      0.94      0.94      1010\n",
            "           4       0.96      0.93      0.95       982\n",
            "           5       0.95      0.94      0.94       892\n",
            "           6       0.96      0.98      0.97       958\n",
            "           7       0.94      0.94      0.94      1028\n",
            "           8       0.97      0.88      0.92       974\n",
            "           9       0.92      0.93      0.93      1009\n",
            "\n",
            "    accuracy                           0.95     10000\n",
            "   macro avg       0.95      0.95      0.95     10000\n",
            "weighted avg       0.95      0.95      0.95     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classification with Support Vector Machine (SVM)\n",
        "\n",
        "In this step, I trained a **Support Vector Machine (SVM)** classifier to recognize handwritten digits from the MNIST dataset.\n",
        "\n",
        "---\n",
        "\n",
        "#### üí° What is SVM?\n",
        "Support Vector Machine is a supervised learning algorithm that finds the best decision boundary (hyperplane) to separate different classes. It works well in high-dimensional spaces and is commonly used for classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚öôÔ∏è Model Used:\n",
        "- **LinearSVC** from `sklearn.svm`\n",
        "- Used a simple **linear kernel** without extensive tuning\n",
        "- Trained on a **subset of 10,000 images** for performance efficiency\n",
        "\n",
        "---\n",
        "\n",
        "#### üìä Results:\n",
        "- Achieved an accuracy of **88.87%**\n",
        "- Performed well on digits like 0, 1, and 6\n",
        "- Struggled more with digits such as 2, 5, and 8\n",
        "- Macro and weighted average F1-score: **~89%**\n",
        "\n",
        "---\n",
        "\n",
        "#### üìå What I Learned:\n",
        "- SVM performs best with the right kernel and parameters (e.g., `C`, `gamma`)\n",
        "- While SVM is fast at prediction, it may underperform without tuning\n",
        "- Still a powerful choice for text and image classification with more optimization\n",
        "\n"
      ],
      "metadata": {
        "id": "PFW1GtVFy37j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# 1. Reduce dataset size for speed (like we did for kNN)\n",
        "x_train_sample = x_train_flat[:10000]\n",
        "y_train_sample = y_train[:10000]\n",
        "\n",
        "# 2. Initialize and train the SVM model\n",
        "svm = LinearSVC()\n",
        "svm.fit(x_train_sample, y_train_sample)\n",
        "\n",
        "# 3. Predict on test set\n",
        "y_pred_svm = svm.predict(x_test_flat)\n",
        "\n",
        "# 4. Evaluate the model\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "print(\"SVM Accuracy:\", accuracy_svm)\n",
        "\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred_svm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "437EZbMYyqIe",
        "outputId": "8964f32b-7fe7-4d00-f799-4ce8145453bd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.8887\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.96      0.95       980\n",
            "           1       0.94      0.97      0.95      1135\n",
            "           2       0.90      0.85      0.87      1032\n",
            "           3       0.86      0.88      0.87      1010\n",
            "           4       0.89      0.90      0.90       982\n",
            "           5       0.84      0.84      0.84       892\n",
            "           6       0.91      0.93      0.92       958\n",
            "           7       0.90      0.90      0.90      1028\n",
            "           8       0.81      0.82      0.82       974\n",
            "           9       0.87      0.84      0.85      1009\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Comparison: kNN vs SVM\n",
        "\n",
        "After training both k-Nearest Neighbors (kNN) and Support Vector Machine (SVM) on the MNIST dataset, I compared their performance on the same test set.\n",
        "\n",
        "---\n",
        "\n",
        "#### üìä Comparison Table\n",
        "\n",
        "| Metric             | k-Nearest Neighbors (kNN) | Support Vector Machine (SVM) |\n",
        "|--------------------|---------------------------|-------------------------------|\n",
        "| **Accuracy**       | ‚úÖ 94.63%                  | üîπ 88.87%                      |\n",
        "| **Precision (avg)**| 95%                        | 89%                           |\n",
        "| **Recall (avg)**   | 95%                        | 89%                           |\n",
        "| **F1-Score (avg)** | 95%                        | 89%                           |\n",
        "| **Training Speed** | Fast (no training phase)   | Slower (requires convergence) |\n",
        "| **Prediction Speed**| Slow (distance-based)     | Fast                          |\n",
        "| **Best For**       | Small-to-medium datasets   | Medium-to-large datasets      |\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚úÖ Key Observations:\n",
        "\n",
        "- **kNN** performed better across all major classification metrics on this subset of MNIST.\n",
        "- **SVM** struggled particularly with certain digits (e.g., 2, 5, 8), which may require kernel tuning or more training data.\n",
        "- kNN is easier to implement but slower at prediction time.\n",
        "- SVM is more efficient during inference but can underperform without parameter optimization.\n",
        "\n",
        "---\n",
        "\n",
        "#### üìå Conclusion:\n",
        "\n",
        "- **kNN** was the better performer in this case, offering higher accuracy and more balanced classification results.\n",
        "- **SVM** remains a strong option, especially when tuned properly or used with non-linear kernels.\n"
      ],
      "metadata": {
        "id": "nEs1tPIX0V_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Step 4 (Optional): Digit Classification using Convolutional Neural Networks (CNN)"
      ],
      "metadata": {
        "id": "YfQzHot01EVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In this step, I explored how deep learning models like **CNNs** can automatically learn spatial features from image data ‚Äî specifically on the MNIST handwritten digit dataset.\n",
        "\n",
        "#### üß© Key Concepts Covered:\n",
        "- Why CNNs outperform classical ML in image tasks.\n",
        "- Importance of using 2D input shape (28x28x1) instead of flattening.\n",
        "- Automatic feature extraction using convolutional filters.\n",
        "\n",
        "#### üß± Model Architecture:\n",
        "- **Conv2D + MaxPooling** layers to extract and reduce features.\n",
        "- **Flatten** to convert image to a vector.\n",
        "- **Dense** layers for classification.\n",
        "- **Softmax** output layer for digit prediction (0‚Äì9).\n",
        "\n",
        "#### ‚öôÔ∏è Training Setup:\n",
        "- Used `categorical_crossentropy` loss and `adam` optimizer.\n",
        "- Trained on 60,000 training images, tested on 10,000.\n",
        "- Reached **~98% accuracy** within just 5 epochs.\n",
        "\n",
        "#### üß™ Testing the Model:\n",
        "- Built a tool to test the trained CNN using custom input images.\n",
        "- Uploaded a handwritten digit image, resized and normalized it.\n",
        "- Model predicted the digit along with a **confidence score** for each class.\n",
        "\n",
        "This optional step gave me hands-on experience with deep learning and demonstrated the power of CNNs in visual recognition tasks.\n"
      ],
      "metadata": {
        "id": "SXw_pIb76l8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Same Data Again\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Check shapes\n",
        "print(\"Training data shape:\", x_train.shape)  # (60000, 28, 28)\n",
        "print(\"Test data shape:\", x_test.shape)       # (10000, 28, 28)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOdKjbeZ1GOo",
        "outputId": "597b3727-596b-47ba-8b2d-ef76756a0258"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (60000, 28, 28)\n",
            "Test data shape: (10000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Reshape (add a channel dimension: grayscale ‚Üí 1 channel)\n",
        "x_train_cnn = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test_cnn = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# 2. Normalize pixel values to [0, 1]\n",
        "x_train_cnn = x_train_cnn / 255.0\n",
        "x_test_cnn = x_test_cnn / 255.0\n",
        "\n",
        "# 3. One-hot encode the labels (0‚Äì9 ‚Üí 10 categories)\n",
        "y_train_cnn = to_categorical(y_train)\n",
        "y_test_cnn = to_categorical(y_test)\n",
        "\n",
        "# Confirm shape\n",
        "print(\"x_train_cnn:\", x_train_cnn.shape)  # (60000, 28, 28, 1)\n",
        "print(\"y_train_cnn:\", y_train_cnn.shape)  # (60000, 10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OIvBY4T1Hqb",
        "outputId": "eca21fe0-2c43-4614-f0c4-163d1729f18f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train_cnn: (60000, 28, 28, 1)\n",
            "y_train_cnn: (60000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')  # 10 classes for digits 0‚Äì9\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train_cnn, y_train_cnn,\n",
        "    validation_data=(x_test_cnn, y_test_cnn),\n",
        "    epochs=5,\n",
        "    batch_size=128\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12jbnkXF1jbl",
        "outputId": "d722e5df-7ac0-43fb-a037-46bc2bd1b93a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 106ms/step - accuracy: 0.8654 - loss: 0.4709 - val_accuracy: 0.9807 - val_loss: 0.0601\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 105ms/step - accuracy: 0.9823 - loss: 0.0615 - val_accuracy: 0.9860 - val_loss: 0.0435\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 104ms/step - accuracy: 0.9883 - loss: 0.0383 - val_accuracy: 0.9886 - val_loss: 0.0323\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 106ms/step - accuracy: 0.9904 - loss: 0.0295 - val_accuracy: 0.9888 - val_loss: 0.0318\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 112ms/step - accuracy: 0.9932 - loss: 0.0210 - val_accuracy: 0.9886 - val_loss: 0.0367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Upload image\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "SnTWI3Kn2VVA",
        "outputId": "dc62a22c-6106-41b0-f09c-bac66bb95ddd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1a93dc40-e83d-4643-9e1e-9390bdefe171\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1a93dc40-e83d-4643-9e1e-9390bdefe171\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Screenshot 2025-07-20 2235082.png to Screenshot 2025-07-20 2235082.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process the image\n",
        "filename = next(iter(uploaded))  # Get uploaded filename\n",
        "img = Image.open(filename).convert('L')  # Convert to grayscale\n",
        "\n",
        "# Resize to 28x28 if needed\n",
        "img = img.resize((28, 28))\n",
        "\n",
        "# Invert colors (if white digit on black)\n",
        "img = np.invert(np.array(img))\n",
        "\n",
        "# Normalize and reshape\n",
        "img = img / 255.0\n",
        "img = img.reshape(1, 28, 28, 1)\n",
        "\n",
        "# Show image\n",
        "plt.imshow(img.reshape(28, 28), cmap='gray')\n",
        "plt.title(\"Preprocessed Input\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "fAVuwzNm3tLA",
        "outputId": "83a342af-7039-4d6d-9aab-6631eb6490e5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities\n",
        "prediction = model.predict(img)\n",
        "\n",
        "# Get predicted class\n",
        "predicted_digit = np.argmax(prediction)\n",
        "confidence = prediction[0][predicted_digit] * 100\n",
        "\n",
        "print(f\"Predicted digit: {predicted_digit} with {confidence:.2f}% confidence\")\n",
        "\n",
        "# Show full probability distribution (optional)\n",
        "for i, prob in enumerate(prediction[0]):\n",
        "    print(f\"{i}: {prob:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBCIpMgg4H_r",
        "outputId": "b703f754-2fbb-4c71-e649-76ce845fb1fd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "Predicted digit: 1 with 25.87% confidence\n",
            "0: 0.0882\n",
            "1: 0.2587\n",
            "2: 0.1386\n",
            "3: 0.1310\n",
            "4: 0.0729\n",
            "5: 0.1011\n",
            "6: 0.0504\n",
            "7: 0.0649\n",
            "8: 0.0340\n",
            "9: 0.0601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3zUTcTSB4_iA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}